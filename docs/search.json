[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I have worked as a Machine Learning Engineer for the last 4 years. As part of my professional experience, my work has generally revolved around the BFSI industry.\nMy current work and interest involves tabular data and computer vision problems."
  },
  {
    "objectID": "posts/2024-07-24-the-data-monitoring-system/index.html",
    "href": "posts/2024-07-24-the-data-monitoring-system/index.html",
    "title": "Monitor your data using Dockerized PostgreSQL, Airflow & Apache Superset",
    "section": "",
    "text": "The Data You Know; The Story You Don’t"
  },
  {
    "objectID": "posts/2024-07-24-the-data-monitoring-system/index.html#tldr",
    "href": "posts/2024-07-24-the-data-monitoring-system/index.html#tldr",
    "title": "Monitor your data using Dockerized PostgreSQL, Airflow & Apache Superset",
    "section": "TL;DR",
    "text": "TL;DR\nIn this blogpost I talk about a data monitoring system that I built to monitor the quality and availibility of data in real time. The system uses Airflow to schedule jobs, PostgreSQL to store data and Superset to visualize the data and monitor its quality.\nThrough this system I monitor the data availability, quality, consistency, and drift. This system enables me to take actions like - identifying discrepancies in the ETL pipeline, if expected data is missing, anomaly in the data that is causing the business objective to fail, and many more."
  },
  {
    "objectID": "posts/2024-07-24-the-data-monitoring-system/index.html#outline",
    "href": "posts/2024-07-24-the-data-monitoring-system/index.html#outline",
    "title": "Monitor your data using Dockerized PostgreSQL, Airflow & Apache Superset",
    "section": "Outline",
    "text": "Outline\nI am assuming that you have Docker installed in your system. If not installed, follow the instructions at .\nWe will:\n\nInstall Airflow, PostgreSQL and Superset with Docker.\nFetch data, preprocess and push to PostgreSQL using Airflow.\nBuild dashboards in Superset to monitor and visualize the data."
  },
  {
    "objectID": "posts/2024-07-24-the-data-monitoring-system/index.html#high-level-architecture-in-a-diagram",
    "href": "posts/2024-07-24-the-data-monitoring-system/index.html#high-level-architecture-in-a-diagram",
    "title": "Monitor your data using Dockerized PostgreSQL, Airflow & Apache Superset",
    "section": "High level architecture in a diagram",
    "text": "High level architecture in a diagram"
  },
  {
    "objectID": "posts/2024-07-24-the-data-monitoring-system/index.html#setting-up-postgresql-airflow-and-superset",
    "href": "posts/2024-07-24-the-data-monitoring-system/index.html#setting-up-postgresql-airflow-and-superset",
    "title": "Monitor your data using Dockerized PostgreSQL, Airflow & Apache Superset",
    "section": "Setting up PostgreSQL, Airflow and Superset",
    "text": "Setting up PostgreSQL, Airflow and Superset\n\nTo setup Airflow using Docker:\n\nDownload Airflow docker-compose\ncurl -LfO 'https://airflow.apache.org/docs/apache-airflow/2.4.1/docker-compose.yaml'\n\n\nCreate mount directories for Docker\nmkdir -p ./dags ./logs ./plugins\n\n\nSave AIRFLOW_UID in .env\necho -e \"AIRFLOW_UID=$(id -u)\" &gt; .env\n\n\nInitialize the database\ndocker-compose up airflow-init\n\n\nStart all airflow services\ndocker-compose up\n\n\n\nTo setup PostgreSQL using Docker:\n\nPull postgres Docker images\ndocker pull postgres\n\n\nRun postgres container\ndocker run --name postgresql -e POSTGRES_USER=&lt;username&gt; -e POSTGRES_PASSWORD=&lt;password&gt; -p 5432:5432 -v /data:/var/lib/postgresql/data -d postgres\n\npostgresql is the name of the Docker Container. -e POSTGRES_USER is the parameter that sets a unique username to the Postgres database. -e POSTGRES_PASSWORD is the parameter that allows you to set the password of the Postgres database. -p 5432:5432 is the parameter that establishes a connection between the Host Port and Docker Container Port. In this case, both ports are given as 5432, which indicates requests sent to the Host Ports will automatically redirect to the Docker Container Port. In addition, 5432 is also the same port where PostgreSQL will be accepting requests from the client. -v is the parameter that synchronizes the Postgres data with the local folder. This ensures that Postgres data will be safely present within the Home Directory even if the Docker Container is terminated. -d is the parameter that runs the Docker Container in the detached mode, i.e., in the background. If you accidentally close or terminate the Command Prompt, the Docker Container will still run in the background. postgres is the name of the Docker image that was previously downloaded to run the Docker Container.\n\n\n\n\nTo setup Superset using Docker:\n\nClone the repository\ngit clone https://github.com/apache/superset.git\ncd superset\n\n\nRun superset\ndocker-compose -f docker-compose-non-dev.yml pull\ndocker-compose -f docker-compose-non-dev.yml up\n\n\n\nTo setup using setup.sh:\nClone the repo (https://github.com/Sharad30/data-monitoring-system) and run:\n./setup.sh"
  },
  {
    "objectID": "posts/2024-07-24-the-data-monitoring-system/index.html#how-do-you-schedule-a-task-using-airflow",
    "href": "posts/2024-07-24-the-data-monitoring-system/index.html#how-do-you-schedule-a-task-using-airflow",
    "title": "Monitor your data using Dockerized PostgreSQL, Airflow & Apache Superset",
    "section": "How do you schedule a task using Airflow?",
    "text": "How do you schedule a task using Airflow?\nApache Airflow is an open-source platform for developing, scheduling, and monitoring batch-oriented workflows.\n\nCreate a python module inside dags folder.\nDefine a function that is executed as a task.\nDefine a DAG to schedule tasks.\nDefine a task using the function defined in step 2.\nSetup task dependencies.\n\nThe below steps ensure that you have an up and running job scheduled at regular intervals:\n\nDefine a function to fetch data from the API or from S3 or any other source\n\ndef download_api_data():\n    print(f\"Fetching data....\")\n    response = requests.get(&lt;API url&gt;)\n    data = response.json()\n    print(f\"Total number of data: {len(data)}\")\n    json_object = json.dumps(data, indent=2)\n    with open(f\"/tmp/pdl_{currency}_hourly.json\", \"w\") as f:\n        f.write(json_object)\n    print(f\"Finished downloading data.....\")\nThe above function fetches data from the API and then stores it as json file for further processing.\n\nDefine a function to move downloaded data(json, csv etc.) to PostgreSQL\n\ndef move_pdl_data_to_postgres(**kwargs):\n    currency = kwargs[\"currency\"]\n    print(f\"Populating for {currency} has started\")\n    with open(f\"/tmp/pdl_{currency}_hourly.json\") as f:\n        data = json.load(f)\n    df = pd.DataFrame(data)\n\n    # Define your preprocessing steps here like typecasting a column according to the Postgresql schema and any other steps specific to your use case\n\n    print(\"All values created, starting the push to db\")\n    df.to_sql(name=\"&lt;name-of-sql-table&gt;\", con=engine, index=False, if_exists=\"append\", chunksize=300)\nIn the above function we load the json data downloaded in step 1 inside a dataframe and then move it to PostgreSQL table, defined in our con parameter of to_sql function.\n\nDefine a DAG\n\n\nDAG object is needed to define how we are going to schedule our various tasks.\nHere we pass a string that defines the dag_id, which serves as a unique identifier for your DAG and also description.\nWe also schedule a DAG using schedule_interval parameter to run it at hourly frequency and also provide start_date of the DAG.\nBelow is an example of the DAG definition:\n\ndag = DAG(\n    dag_id=\"data-migration\",\n    description=\"Daily data monitoring pipeline\",\n    schedule_interval=\"0 * * * *\",\n    start_date=datetime(2022, 10, 17),\n)\n\nDefine PythonOperator\n\n\nOperators are tasks that define a unit of work to be done.\nThere are manny different kind of operators that you can play around with in Airflow. But we will stick to PythonOperator, which takes python function as a parameter.\nHere we define the task_id, python_callable and above defined dag object.\nBelow is how we define the PythonOperator object\n\nPythonOperator(\n            task_id=f\"download_json\",\n            python_callable=download_json,\n            dag=dag,\n        )\n\nSetup task dependencies\n\nLets say we have 2 PythonOperator defined as 2 tasks and one task is dependent on the other. In our case we first fetch the data from API and then push the data to PostgreSQL. So setting up task dependency kind of becomes and it is defined by using &gt;&gt; operator as follows:\ntask1 &gt;&gt; task2\nHere the Airflow DAG knows that it has to first finish running the task1 and then move on to task2. Any failure in task1 will result in termination of the job."
  },
  {
    "objectID": "posts/2024-07-24-the-data-monitoring-system/index.html#what-is-the-airflow-ui-going-to-look-like",
    "href": "posts/2024-07-24-the-data-monitoring-system/index.html#what-is-the-airflow-ui-going-to-look-like",
    "title": "Monitor your data using Dockerized PostgreSQL, Airflow & Apache Superset",
    "section": "What is the Airflow UI going to look like?",
    "text": "What is the Airflow UI going to look like?\n\nThe above UI can be accessed after Airflow login and navigating as follows: &lt;DAG-name&gt; &gt; Graph.\nThe Graph shows you the various tasks that are scheduled to run and each row defines multiple tasks and how each one is dependent on the other i.e Task move_pdl_data_to_postgres_ADA is dependent on download_pdl_json_ADA and hence has to be completed first.\nThe subsequent rows follow a similar pattern and here we have demonstrated multiple different jobs scheduled inside a single DAG, where each job does the same thing as other, but for different type of data i.e for different bitcoin currencies in our scenario."
  },
  {
    "objectID": "posts/2024-07-24-the-data-monitoring-system/index.html#how-to-visualize-the-raw-data-in-apache-superset",
    "href": "posts/2024-07-24-the-data-monitoring-system/index.html#how-to-visualize-the-raw-data-in-apache-superset",
    "title": "Monitor your data using Dockerized PostgreSQL, Airflow & Apache Superset",
    "section": "How to visualize the raw data in Apache Superset?",
    "text": "How to visualize the raw data in Apache Superset?\n\n\nSuperset is a data exploration and visualization platform and we are going to leverage it to use it as our frontend for monitoring the data we move to the PostgreSQL at regular intervals.\nAs seen in the above example dashboard we are doing some sanity check and checking the trend for a bitcoin currency.\nSo playing around with visualizations specific to your data and problem statement is straight forward in Superset and it comes with a bunch of features."
  },
  {
    "objectID": "posts/2024-07-24-the-data-monitoring-system/index.html#what-next",
    "href": "posts/2024-07-24-the-data-monitoring-system/index.html#what-next",
    "title": "Monitor your data using Dockerized PostgreSQL, Airflow & Apache Superset",
    "section": "What next?",
    "text": "What next?\nThis task can be further expanded in various aspects each from PostgreSQl, Airflow and Superset perspective, by adding more sources of information that we want to monitor in real time and keep adding more tables to our PostgreSQL database, schedule more DAGs in our Airflow container and add more dashboards monitoring the nature of different data."
  },
  {
    "objectID": "posts/2024-07-24-the-synthetic-data-story/index.html",
    "href": "posts/2024-07-24-the-synthetic-data-story/index.html",
    "title": "The Synthetic Data Story",
    "section": "",
    "text": "The Data You Know; The Story You Don’t"
  },
  {
    "objectID": "posts/2024-07-24-the-synthetic-data-story/index.html#what-is-sdv",
    "href": "posts/2024-07-24-the-synthetic-data-story/index.html#what-is-sdv",
    "title": "The Synthetic Data Story",
    "section": "What is SDV?",
    "text": "What is SDV?\nThe Synthetic Data Vault (SDV) is a Synthetic Data Generation ecosystem of libraries that allows users to generate new Synthetic Data that has the same format and statistical properties as the original dataset.\nThe library can also cater to different nature of data as below: 1. single-table - Used to model single table datasets. 2. multi-table - Used to model relational datasets. 3. timeseries - Used to model time-series datasets.\nFeature Highlights\n\nSynthetic data generators for single tables with the following features:\n\nUsing Copulas and Deep Learning based models.\nHandling of multiple data types and missing data with minimum user input.\nSupport for pre-defined and custom constraints and data validation.\n\nSynthetic data generators for complex multi-table, relational datasets with the following features:\n\nDefinition of entire multi-table datasets metadata with a custom and flexible JSON schema.\nUsing Copulas and recursive modeling techniques.\n\nSynthetic data generators for multi-type, multi-variate timeseries with the following features:\n\nUsing statistical, Autoregressive and Deep Learning models.\nConditional sampling based on contextual attributes."
  },
  {
    "objectID": "posts/2024-07-24-the-synthetic-data-story/index.html#getting-started-with-sdv",
    "href": "posts/2024-07-24-the-synthetic-data-story/index.html#getting-started-with-sdv",
    "title": "The Synthetic Data Story",
    "section": "Getting started with SDV",
    "text": "Getting started with SDV\npip install sdv"
  },
  {
    "objectID": "posts/2024-07-24-the-synthetic-data-story/index.html#code",
    "href": "posts/2024-07-24-the-synthetic-data-story/index.html#code",
    "title": "The Synthetic Data Story",
    "section": "Code",
    "text": "Code\n\nimport pandas as pd\nfrom sdv.timeseries import PAR\nimport altair as alt\n\n/home/sharad/repo/projects/data-glance/.venv/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\nLoad synthetic data\n\n\ndf = pd.read_csv(\"../data/synthetic.csv\")\ndf.loc[:, \"date\"] = pd.to_datetime(df.date)\n\n\ndf.head()\n\n\n\n\n\n\n\n\nwebsiteName\ncategory\ncountry\nstate\ncity\nos\ndevice\nproduct\ndate\ncount\n\n\n\n\n0\nokaz.com.sa\nsociety&culture\nsaudi arabia\nar riyad\nriyadh\niOS\nmobile\nRotatingCube\n2022-05-01 00:00:00\n545\n\n\n1\nokaz.com.sa\nsociety&culture\nsaudi arabia\nar riyad\nriyadh\niOS\nmobile\nRotatingCube\n2022-05-01 01:00:00\n884\n\n\n2\nokaz.com.sa\nsociety&culture\nsaudi arabia\nar riyad\nriyadh\niOS\nmobile\nRotatingCube\n2022-05-01 02:00:00\n708\n\n\n3\nokaz.com.sa\nsociety&culture\nsaudi arabia\nar riyad\nriyadh\niOS\nmobile\nRotatingCube\n2022-05-01 03:00:00\n550\n\n\n4\nokaz.com.sa\nsociety&culture\nsaudi arabia\nar riyad\nriyadh\niOS\nmobile\nRotatingCube\n2022-05-01 04:00:00\n271\n\n\n\n\n\n\n\n\nLets understand how we go about defining our auto-regressive time series model to model our data. There are 2 key parameters that would specifically define how our model is going to learn from our time series data.\nAs mentioned before about the heirarchical nature of the data, we would define our PAR model parameters according to that:\n\nentity_columns: This uniquely defines the entity for which we want to generate a time series. In our case the entity is not singular i.e we want time series for each unique group comprising of websiteName, category, country, state, city, os, device, product.\nsequence_index: Datetime column for time series reference.\n\n\n\nentity_columns = ['category', 'country', 'state', 'city', 'os', 'device', 'product', 'websiteName']\nsequence_index = 'date'\n\nmodel = PAR(\n            entity_columns=entity_columns,\n            sequence_index=sequence_index,\n           )\n\nFit PAR model on the original data\n\n%%time\nmodel.fit(df)\n\nCPU times: user 28.4 s, sys: 154 ms, total: 28.5 s\nWall time: 11 s\n\n\nConditional Sampling\n\nThere is one key feature that PAR model provides, which we are going to specifically use for our problem, that is instead of generating random samples from the model based on raw data, we can provide specific context in which we want the samples.\nThis can be achieved by defining a dataframe consisting of unique entities for which we want to generate a time-series for. One thing to note here is that, the raw data we provided to the model, has 24 hours of data for each unique entity. So the multiple contexts that we define in the next step, the model is going to output 24 rows for each entity.\n\n\ncontext_columns = ['category', 'country', 'state', 'city', 'os', 'device', 'product', 'websiteName']\ndf_freq = df.groupby(context_columns).date.count().sort_values(ascending=False).reset_index()\nmost_freq_group = df_freq[context_columns].to_dict(orient=\"records\")\n\ncontext = pd.DataFrame(\n   most_freq_group\n)\n\n\ncontext\n\n\n\n\n\n\n\n\ncategory\ncountry\nstate\ncity\nos\ndevice\nproduct\nwebsiteName\n\n\n\n\n0\narts&entertainment\nsaudi arabia\nar riyad\nriyadh\niOS\nmobile\nRotatingCube\nokaz.com.sa\n\n\n1\narts&entertainment\nsaudi arabia\nar riyad\nriyadh\niOS\nmobile\nvibe\nokaz.com.sa\n\n\n2\nsociety&culture\nsaudi arabia\nmakkah al mukarramah\njeddah\niOS\nmobile\nimpulse\nokaz.com.sa\n\n\n3\nsociety&culture\nsaudi arabia\nmakkah al mukarramah\njeddah\niOS\nmobile\nRotatingCube\nokaz.com.sa\n\n\n4\nsociety&culture\nsaudi arabia\nar riyad\nriyadh\niOS\nmobile\nvibe\nokaz.com.sa\n\n\n5\nsociety&culture\nsaudi arabia\nar riyad\nriyadh\niOS\nmobile\nimpulse\nokaz.com.sa\n\n\n6\nsociety&culture\nsaudi arabia\nar riyad\nriyadh\niOS\nmobile\nRotatingCube\nokaz.com.sa\n\n\n7\nnews\nsaudi arabia\nmakkah al mukarramah\njeddah\niOS\nmobile\nvibe\nokaz.com.sa\n\n\n8\nnews\nsaudi arabia\nmakkah al mukarramah\njeddah\niOS\nmobile\nimpulse\nokaz.com.sa\n\n\n9\nnews\nsaudi arabia\nmakkah al mukarramah\njeddah\niOS\nmobile\nRotatingCube\nokaz.com.sa\n\n\n10\nnews\nsaudi arabia\nar riyad\nriyadh\niOS\nmobile\nvibe\nokaz.com.sa\n\n\n11\nnews\nsaudi arabia\nar riyad\nriyadh\niOS\nmobile\nimpulse\nokaz.com.sa\n\n\n12\nnews\nsaudi arabia\nar riyad\nriyadh\niOS\nmobile\nRotatingCube\nokaz.com.sa\n\n\n13\nhealth&fitness\nindonesia\njawa barat\nbekasi\nAndroid\nmobile\nimpulse\nsajiansedap.grid.id\n\n\n14\nbusiness&finance\nsaudi arabia\nmakkah al mukarramah\njeddah\niOS\nmobile\nRotatingCube\nokaz.com.sa\n\n\n15\nbusiness&finance\nsaudi arabia\nar riyad\nriyadh\niOS\nmobile\nvibe\nokaz.com.sa\n\n\n16\nbusiness&finance\nsaudi arabia\nar riyad\nriyadh\niOS\nmobile\nRotatingCube\nokaz.com.sa\n\n\n17\narts&entertainment\nsaudi arabia\nmakkah al mukarramah\njeddah\niOS\nmobile\nvibe\nokaz.com.sa\n\n\n18\narts&entertainment\nsaudi arabia\nmakkah al mukarramah\njeddah\niOS\nmobile\nRotatingCube\nokaz.com.sa\n\n\n19\nsociety&culture\nsaudi arabia\nmakkah al mukarramah\njeddah\niOS\nmobile\nvibe\nokaz.com.sa\n\n\n\n\n\n\n\nGenerate synthetic data\n\ndf_synthesized = model.sample(context=context)\n\n\ndf_synthesized.shape\n\n(480, 10)\n\n\n\nAs we passed 20 contexts to the model and we were expecting 24 rows for each context, the size of synthesize data is therfore 20 * 24 = 480\n\n\ndf_synthesized.loc[:, \"group_id\"] = df_synthesized.groupby(['category', 'country', 'state', 'city', 'os', 'device', 'product', 'websiteName']).ngroup().apply(lambda x: str(x) + \"_syn\")\n\ndf.loc[:, \"group_id\"] = df.groupby(['category', 'country', 'state', 'city', 'os', 'device', 'product', 'websiteName']).ngroup().apply(lambda x: str(x) + \"_og\")\n\n\nFor the purpose ofo visualizing the raw and model generated data, we create unique group-ids for both raw(groupid_og) and synthetic(groupid_syn) data\n\n\ndef plot_timeseries(df):\n    # select a point for which to provide details-on-demand\n    label = alt.selection_single(\n        encodings=['x'], # limit selection to x-axis value\n        on='mouseover',  # select on mouseover events\n        nearest=True,    # select data point nearest the cursor\n        empty='none'     # empty selection includes no data points\n    )\n\n    # define our base line chart of stock prices\n    base = alt.Chart(df).mark_line().encode(\n        alt.X('date:T'),\n        alt.Y('count:Q'),\n        alt.Color('group_id:N')\n    )\n\n    return alt.layer(\n        base, # base line chart\n\n        # add a rule mark to serve as a guide line\n        alt.Chart().mark_rule(color='#aaa').encode(\n            x='date:T'\n        ).transform_filter(label),\n\n        # add circle marks for selected time points, hide unselected points\n        base.mark_circle().encode(\n            opacity=alt.condition(label, alt.value(1), alt.value(0))\n        ).add_selection(label),\n\n        # add white stroked text to provide a legible background for labels\n        base.mark_text(align='left', dx=5, dy=-5, stroke='white', strokeWidth=2).encode(\n            text='count:Q'\n        ).transform_filter(label),\n\n        # add text labels for stock prices\n        base.mark_text(align='left', dx=5, dy=-5).encode(\n            text='count:Q'\n        ).transform_filter(label),\n\n        data=df\n    ).properties(\n        width=500,\n        height=400\n    )\n\nGroupwise OG vs Synthetic time-series\n\ndf_vis = pd.concat([df[df.group_id.isin([\"0_og\"])], df_synthesized[df_synthesized.group_id.isin([\"0_syn\"])]], ignore_index=True)\nplot_timeseries(df_vis)\n\n\n\n\n\n\n\ndf_vis = pd.concat([df[df.group_id.isin([\"1_og\"])], df_synthesized[df_synthesized.group_id.isin([\"1_syn\"])]], ignore_index=True)\nplot_timeseries(df_vis)\n\n\n\n\n\n\n\ndf_vis = pd.concat([df[df.group_id.isin([\"2_og\"])], df_synthesized[df_synthesized.group_id.isin([\"2_syn\"])]], ignore_index=True)\nplot_timeseries(df_vis)\n\n\n\n\n\n\n\ndf_vis = pd.concat([df[df.group_id.isin([\"3_og\"])], df_synthesized[df_synthesized.group_id.isin([\"3_syn\"])]], ignore_index=True)\nplot_timeseries(df_vis)\n\n\n\n\n\n\n\ndf_vis = pd.concat([df[df.group_id.isin([\"4_og\"])], df_synthesized[df_synthesized.group_id.isin([\"4_syn\"])]], ignore_index=True)\nplot_timeseries(df_vis)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "sharad.ai",
    "section": "",
    "text": "The Synthetic Data Story\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMonitor your data using Dockerized PostgreSQL, Airflow & Apache Superset\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  }
]